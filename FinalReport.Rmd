---
title: "Final Report"
author: "Noah Estrada-Rand, Brady Hoskins, Charles Filce"
date: "12/12/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(forcats)
library(tidyverse)
library(summarytools)
library(corrplot)
library(ggplot2)
library(glmnet)
library(glmnetUtils)
library(randomForest)
library(randomForestExplainer)
library(caret)
library(plotROC)
library(PRROC)
library(gmodels)
library(tree)


####cleaning the data 
steam <- read.csv("steam.csv")
nrow(steam[!complete.cases(steam),])
steam$price <- steam$price *1.28
steam<- subset(steam, select = -c(appid,english,steamspy_tags,
                                  name,release_date,
                                  platforms,publisher,developer))
steam$genres <- do.call('rbind',strsplit(as.character(steam$genres), ';', fixed=TRUE))[,1]
steam$categories <- do.call('rbind',strsplit(as.character(steam$categories), ';', fixed=TRUE))[,1]
steam$categories <- as.factor(steam$categories)
steam$genres <- as.factor(steam$genres)
steam <- steam[steam$average_playtime < 100000,]
steam <- steam[steam$price <50,]
steam <- steam[steam$average_playtime < 40000,]
steam <- steam[steam$negative_ratings < 2e+05,]
steam <- steam[steam$positive_ratings < 1e+06,]
steam$simple_categories <- fct_lump(steam$categories,n = 4)
steam$categories <- ifelse(steam$categories == "Single-player","SinglePlayer",
                           ifelse(steam$categories == "Multi-player","Multi-Player",
                                  ifelse(steam$categories == "Online Multi-Player","Multi-Player",
                                         ifelse(steam$categories == "Local Multi-Player","Multi-Player",
                                                ifelse(steam$categories == "MMO","MMO",
                                                       ifelse(steam$categories == "Co-op","Co-op",
                                                              ifelse(steam$categories == "Shared/Split Screen","Co-op",
                                                                     ifelse(steam$categories == "Local Co-op","Co-op",
                                                                            ifelse(steam$categories == "Online Co-op","Co-op",
                                                                                   ifelse(steam$categories == "Steam Cloud","Steam",
                                                                                          ifelse(steam$categories == "Steam Trading Cards","Steam",
                                                                                                 ifelse(steam$categories == "Steam Leaderboards","Steam",
                                                                                                        ifelse(steam$categories == "Steam Achievements","Steam","Other")))))))))))))
steam$categories <- as.factor(steam$categories)
steam$genres <- fct_lump(steam$genres,n = 5)
steam$successfulGame <- ifelse(steam$owners == "10000000-20000000",1,
                               ifelse(steam$owners == "20000000-50000000",1,
                                      ifelse(steam$owners == "50000000-100000000",1,
                                             ifelse(steam$owners == "100000000-200000000",1,
                                                    ifelse(steam$owners == "5000000-10000000",1,
                                                           ifelse(steam$owners == "2000000-5000000",1,
                                                                  ifelse(steam$owners == "1000000-2000000",1,0)))))))

steam <- subset(steam, select = -c(owners,categories,positive_ratings,negative_ratings,
                                   median_playtime))
steam$required_age <- as.factor(steam$required_age)
```

# Business Problem
Our group was interested in finding what characteristics of video games lead to a successful game. Our results would give developers an insight into what specific features should be focused on when developing their games to maximize success. Since there are many different variables within each game, we looked to find key indicators of a successful game.

# Motivation
The gaming industry is projected to reach an annual revenue of $230 billion dollars by the year 2020. Since the gaming industry is quickly expanding into one of the highest grossing industries in the world, we wanted to see what types of games are pushing this industry to the top. When looking at different successful games in our data set, at times there can be a large disconnect between the resources a game was developed with and the total number of copies sold. Because of this we wanted to analyze what specific features led to the less well known games being considered as successful as the bigger, more popular games.

# Summary Statistics
The Steam Store dataset consisted of 27,075 observations, or different games sold on the Steam store, and 18 different features, or characteristics of the games. This data set was provided by scraping data from the Steam store by a third party service, Steam Spy. Our data set consisted of 7 numeric variables: Required Age, Number of Achievements, Positive Ratings, Negative Ratings, Average Playtime, Median Playtime, and Price. The dataset also consisted of 11 factored variables: Application Id, Name, Release Date, English, Developer, Publisher, Platform, Categories, Genres, Steam Spy set, and Number Of Owners. We also created 2 factored variables: Simple_Categories, and SuccessfulGame.
Ultimately we kept only the variables of required_age, genres, achievements, average_playtime, price, simple_categories, and successfulGame.  
```{r}
head(steam,5)
```
The drastic decrease in number of variables is due to the fact that many of the variables proved to be unusable in their current state.  Many of them contained three semicolon delimited values, ultimately making it difficult to distinguish a discernable category or value for that particular variable.  Moreover, in regards to variables such as publisher and developer, it was not possible to build meaningful evaluations off of the data as there were over 10,000 different possible classes. As such, we used the variables that were still impactful and meaningful to our analyses.
From these remaining variables, the following summary statistics were calculated
```{r}
descr(steam)
```
From the table above there are a few significant things to point out. Our data produced a mean of .02 for the number of successful games. This is significant because it points out that the overwhelming majority of the games in this data set we consider to be unsuccessful games. There were some extreme outliers for achievements and average playtime, so when we ran our models we removed these observations from the dataset to prevent skewing our results. We also found it very interesting that the number of achievements in games was rather low, and it would most likely drop a significant amount if we had removed the outliers when we generated this table. The standard deviations for average playtime and achievements is very high, suggesting that there is a large amount of variation in our data set.
```{r,echo = FALSE}
numeric_cols <- sapply(steam,is.numeric)
correlations <- cor(steam[,numeric_cols])
corrplot(correlations)
```
From the correlation matrix above you can see that there are 2 variables that are correlated with a successful game: price and average playtime. This made sense to us for two main reasons: the first is that if a games price is low or free it is more accessible to the public, and the second reason is that if a game has a high average playtime there must be a lot of content in the game that keeps its players for long periods of time. If the game is more accessible to the public then more players will be drawn to it, thus increasing popularity and the potential for players to buy in game cosmetics or expansions. If a game has a high average playtime, it shows that there is enough content to keep players interested in the game. To us average playtime provided a lot of information into a games retention rate of its players, if the retention rate was higher it is more likely that the game is successful.
# Summary Plots
```{r,echo=FALSE}
ggplot(steam,aes(x = achievements,y = average_playtime)) + geom_point(aes(color = successfulGame))+
  geom_smooth() + labs(x = "Number of In-Game Achievements",y = "Average Playtime (hrs)",
                       title = "Plotted Achievements and Average Playtime")
```
In looking to find impactful variables to base our analyses on, we initially believed that more achievements in a game would lead to higher average playtime. This was not the case, however, as the plot above clearly shows that some of the games with the lowest achievements have the highest average playtime.
```{r,echo=FALSE}
ggplot(steam,aes(x = price,y = average_playtime)) + 
  geom_point(aes(color = successfulGame)) + 
  labs(x = "Price of the Game (dollars)",y ="Average Playtime (hrs)",
       title = "Average Playtime Plotted Against Price of Game")
```
From the plot above it becomes clear that price does not necessarily translate to higher average playtime. This is characterized by the equal dispersion of high average playtimes amongst the different price ranges.  Furthermore, it is intersting to notice that successfulgames are also equally distributed among the varying price ranges.
```{r,echo = FALSE}
boxplot(steam$average_playtime~steam$required_age,
        main = "Average Playtime by Required Age",xlab = "Required Age (years)",
        ylab = "Average Playtime")
```
As seen in the plot above, it appears that games with essentially no age requirement have a much broader range of average playtimes than any other level of required age.  This intuitively makes sense because the more people that can play a game, the more the game will be played, and thus a higher average playtime overall.
```{r,echo = FALSE}
ggplot(steam,aes(x = genres,y = price)) + geom_boxplot() + 
  labs(title = "Boxplots of Price By Genre",x = "Genres",y = "Price (dollars)")
```
The boxplot above shows the distribution of prices for video games against the different kinds of genres.  While many of the genres have similar ranges, it is interesting to notice that casual games clearly have the lowest mean price and range of prices.  Moreover, it is important to notice the "other" and "indie" categories as having th widest range of prices, perhaps due to the fact that many of these games are perhaps created by independent studios, leading them to demand higher prices as they are unable to produce the games for lower costs.

# Models

In looking to fit models to help solve our business problem, we decided to use logistic regression, random forest, and tree models. 
## Logistic Regreesion

In building our logistic regression model, we first attempted using all variables as predictors of our successfulGame variable.  From this initial model we were able to see which variables were statistically significant and proceeded to use these significant variables to build a new logistic model.  The variables used as predictors were price, genres, required age, and average playtime.
```{r,echo = FALSE}
set.seed(2019)
train_index <- sample(1:nrow(steam),.75*nrow(steam),replace = FALSE)
steam_train <- steam[train_index,]
steam_test <- steam[-train_index,]
steam_logit <- glm(successfulGame~price+relevel(genres,ref = "Other") + required_age+
                     average_playtime,
                   data = steam_train,
                   family = "binomial")
steam_train$logit_preds <- predict(steam_logit,type = "response")
steam_test$logit_preds <- predict(steam_logit,newdata = steam_test,
                                  type = "response")
steam_train$pred_class <- ifelse(steam_train$logit_preds >.04,1,0)
steam_test$pred_class <-ifelse(steam_test$logit_preds>.04,1,0)
```
```{r}
exp(steam_logit$coefficients)
```
Looking at price, the model suggests that for every unit increase in price, a dollar, the likelihood of the game being successful rises by three percent.  Moreover, looking at the different genres of games, it becomes clear that any game not in the “Other” category leads to a decrease in likelihood of a game being successful.  Lastly, when considering the coefficients for required age, it is interesting to notice that a game being or required age 12, 16, or 18 leads to a significant increase in the likelihood of a game being successful.  
We interpreted the coefficients above as indicating that having a game considered in the “Other” category as well as making the age requirement higher leads to the largest increases in likelihood of a game being successful.  However, it is very important to take this interpretation lightly as we must also consider what other variables may play into this effect.  In the case of age requirements, perhaps it is the more mature content of the video games, and not necessarily the age requirement itself that drives the increase in likelihood of a game being successful or not.  Moreover, a game being considered in the “Other” genre may increase the likelihood of a game succeeding as it breaks the traditional genre molds of most games, indicative of innovative and novel game development.
```{r,echo = FALSE}

train_ROC <- ggplot(steam_train,aes(m = logit_preds,
                                    d = successfulGame)) +
  geom_roc(labelsize = 3.5,
           cutoffs.at = c(.99,.9,.7,.6,.5,.4,.1,.01)) +
  labs(title = "ROC Curve for Train Data",x = "False Positive Fraction",
       y= "True Positive Fraction")
test_ROC <- ggplot(steam_test,aes(m = logit_preds,
                                  d = successfulGame)) +
  geom_roc(labelsize = 3.5,
           cutoffs.at = c(.99,.9,.7,.6,.5,.4,.1,.01)) +
  labs(title = "ROC Curve for Test Data",x = "False Positive Fraction",
       y= "True Positive Fraction")
train_ROC
test_ROC
```
Looking at the ROC plots above, it is clear to see that the optimal cutoff for our logistic model resides between 0.1 and 0, indicative of a very low cutoff to maximize true positives and minimize false positives.
```{r,echo = FALSE}
CrossTable(steam_train$pred_class,steam_train$successfulGame,
           prop.r = FALSE,
           prop.c = FALSE,
           prop.t = FALSE,
           prop.chisq = FALSE)
CrossTable(steam_test$pred_class,steam_test$successfulGame,
           prop.r = FALSE,
           prop.c = FALSE,
           prop.t = FALSE,
           prop.chisq = FALSE)
```
After considering different cutoffs between 0.1 and 0, we decided to use a cutoff of .04 as going any closer to 0 would increase sensitivity while sharply penalizing specificity.  Although the accuracy, and the specificity of this model is relatively strong on both test and training datasets, it becomes clear that there is much to be desired in terms of sensitivity.  With this in mind we set out to improve sensitivity while maintaining the optimal levels of specificity and accuracy.
## RandomForest

	Next, we began to build a random forest model to hopefully increase the sensitivity of our predictions.  In order to do so, we used cross validation to find the best number of variables to consider at each split of each tree.  We plotted the out of bag error against the number of variables tried and found the optimal number to be 2.  With this  in mind we built our random forest model which yielded the insightful results.
```{r,echo = FALSE}
steam$successfulGame <- as.factor(steam$successfulGame)

set.seed(2019)
train_idx <- sample(1:nrow(steam), size = floor(.75 * nrow(steam)))
steam_train <- steam[train_idx,]
steam_test <- steam[-train_idx,]

random_forest_steam <- randomForest(successfulGame~.,
                                    data = steam_train,
                                    mtry = 2,
                                    ntrees = 500,
                                    type = classification,
                                    importance = TRUE)
random_forest_preds <- predict(random_forest_steam,type = "response")
preds_2 <- data.frame(steam_train,preds= random_forest_preds)
preds_test_2 <-data.frame(steam_test,preds = predict(random_forest_steam,
                                                     newdata = steam_test,
                                                     type = "response"))
varImpPlot(random_forest_steam)
```
As seen in the importance plot the variables of average playtime and price are the two most important variables in the 500 trees made in the random forest model.  This largely aligned with the “random forest explained” results that are included outside of this report due to their formatting.  The figures included in that report indicate that average playtime and required age were used as roots the most while also having the lowest average depth in each tree, once again highlighting the most important variables as predictors. 
```{r,echo = FALSE}
CrossTable(preds_2$successfulGame,preds_2$preds,
           prop.r = FALSE,
           prop.c = FALSE,
           prop.t = FALSE,
           prop.chisq = FALSE)
#test confusion matrix
CrossTable(preds_test_2$successfulGame,preds_test_2$preds,
           prop.r = FALSE,
           prop.c = FALSE,
           prop.t = FALSE,
           prop.chisq = FALSE)
```
After making predictions using the random forest model, the above confusion matrices were produced.  Here it is clear that the random forest model was able to maintain the high level of accuracy and specificity while also increasing the sensitivity of the model. An increase of roughly 20-30% in sensitivity was able to make this model much more effective at predicting if a game is successful.  Thus this model was able to outperform the initial logistic regression model.

## Pruned Tree Model

While the random forest model offered us increased performance compared to the logit model, we still looked for the possibility of a simpler, yet equally effective model.  Thus, we fitted a basic tree model using all possible variables as predictors.  We then pruned the tree to find the number of splits to give us the lowest amount of deviance from the truth data, only to find the pruned tree to be identical to the initial tree.  
```{r,echo =FALSE}
steam$successfulGame <- as.factor(steam$successfulGame)

set.seed(2019)
train_idx <- sample(1:nrow(steam), size = floor(.75 * nrow(steam)))
steam_train <- steam[train_idx,]
steam_test <- steam[-train_idx,]
###tree based on the top 4 most important variables
steam_tree <- tree(successfulGame~.,
                   data = steam_train)
###cross validation
tree_cv <- cv.tree(steam_tree)
best_tree_index <- which.min(tree_cv$dev)
best_size <- tree_cv$size[best_tree_index]

####prune the tree
pruned_tree <- prune.tree(steam_tree,best = best_size)
plot(pruned_tree)
text(pruned_tree,pretty=0)
```
The tree model appeared to draw the same conclusions as the random forest model, indicating average playtime and price to be the most important variables to create splits off of.  
```{r,echo =FALSE}
basic_preds_train <- data.frame(steam_train,preds = predict(pruned_tree,
                                                            type = "class"))
basic_preds_test <- data.frame(steam_test,preds = predict(pruned_tree,
                                                          newdata = steam_test,
                                                          type = "class"))
###for train
CrossTable(basic_preds_train$successfulGame,basic_preds_train$preds,
           prop.r = FALSE,
           prop.c = FALSE,
           prop.t = FALSE,
           prop.chisq = FALSE)
###for test
CrossTable(basic_preds_test$successfulGame, basic_preds_test$preds,
           prop.r = FALSE,
           prop.c = FALSE,
           prop.t = FALSE,
           prop.chisq = FALSE)
```
From the tree model, we once again made predictions using the model to observe its performance against the truth data.  Ultimately, we found the tree model to perform similarly to the logistic model in regards to sensitivity, whilst maintaining high accuracy and specificity, just as the other two models had.  Thus, while this model was able to offer a human readable model, it offered lower sensitivity than the random forest model.

# Conclusions














