x <- (1,3,12,42,58,61,71,89,91,98)
x <- c(1,3,12,42,58,61,71,89,91,98)
mean(x)
median(x)
x <- c(4,
28,
31,
63,
66,
68,
72,
72,
74,
77)
x
mean(x)
median(x)
x <- c(19,
27
32
51
56
59
60
66
68
83)
,
x <- c(19,
27,
32,
51,
56,
59,
60,
66,
68,
83)
mean(x)
x <-c(2,
4,
4,
7,
30,
37,
45,
64,
77,
93)
mean(x)
max(x)
median(x)
x <-c(15,
19,
29,
58,
71,
72,
88,
91,
96,
98)
median(x)
x <-c(4,
15,
17,
17,
22,
49,
54,
57,
77,
84)
median(x)
mean(x)
max(x)
library(ggplot2)
library(factoextra)
#using iris data: flowers species
data("iris")
# hierarchical clustering
dissimilarity <- dist(irist[,3:4])
# hierarchical clustering
dissimilarity <- dist(irist[,c(3,4)])
# hierarchical clustering
dissimilarity <- dist(iris[,c(3,4)])
# hierarchical clustering
dissimilarity <- dist(iris[,c(3:4)])
clusters <- hclust(dissimilarity)
clusters
dissimilarity
# plotting the dendogram
plot(clusters)
rect.hclust(clusters,k=3)
# cutting the tree to find clusters
plot(clusters)
rect.hclust(clusters,k=3)
rect.hclust(clusters,k=3,border = 1:3)
rect.hclust(clusters,k=3,border = 1:3)
###cutting the dendrogram
cluster_cut <- cutree(clusters,k=3)
plot(ccluster_cut)
###cutting the dendrogram with specified number of clusters
cluster_cut <- cutree(clusters,k=3)
plot(ccluster_cut)
plot(cluster_cut)
table(ccluster_cut)
table(cluster_cut)
table(cluster_cut,iris$Species)
iris_clusters2 <- hculst(dist(iris[,3:4]),method = 'average')
iris_clusters2 <- hclust(dist(iris[,3:4]),method = 'average')
plot(iris_clusters2)
rect.hclust(iris_clusters2,3)
cluster_cut2 <- cutree(iris_clusters2,3)
table(cluster_cut2,iris$Species)
table(cluster_cut,iris$Species)
table(cluster_cut2,iris$Species)
ggplot(iris,aes(x= Petal.length,y = Petal.width,color = Species)) +
geom_point(alpha = .4,size = 3.5) +
geom_point(col = cluster_cut2)
ggplot(iris,aes(x= Peta.length,y = Petal.width,color = Species)) +
geom_point(alpha = .4,size = 3.5) +
geom_point(col = cluster_cut2)
colnames(iris)
ggplot(iris,aes(x= Petal.Length,y = Petal.width,color = Species)) +
geom_point(alpha = .4,size = 3.5) +
geom_point(col = cluster_cut2)
ggplot(iris,aes(x= Petal.Length,y = Petal.Width,color = Species)) +
geom_point(alpha = .4,size = 3.5) +
geom_point(col = cluster_cut2)
#------------------------------------------------
### Auto data
#------------------------------------------------
# using Auto data
library(ISLR)
data("Auto")
auto
# scaling
####make variables similar scales
str(Auto)
Auto_scale <- scale(Auto[,1:7])
head(Auto)
head(Auto_scale)
fviz_nbclust(Auto_scaled,
kmeans,
method="wss") +
geom_vline(xintercept=3, linetype =2) +
labs(subtitle ="Elbow Method")
Auto_scaled <- scale(Auto[,1:7]) ###gets rid of origin and name and scales the values
#------------------------------------------------
### number of clusters?
#------------------------------------------------
# elbow method
fviz_nbclust(Auto_scaled,
kmeans,
method="wss") +
geom_vline(xintercept=3, linetype =2) +
labs(subtitle ="Elbow Method")
# Silhouette method
fviz_nbclust(Auto_scaled,
kmeans,
method="silhouette")
# Gap Statistic method
fviz_nbclust(Auto_scaled,
kmeans,
method="gap_stat",
nboot=100)
library("NbClust")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "eucladian")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "euclidean")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "eucledean")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "eucladean")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "euclidean",
min.nc = 2,
max.nc = 10,
method = "kmeans")
Auto_scaled <- scale(Auto[,1:7]) ###gets rid of origin and name and scales the values
Nb_cl <- NbClust(Auto_scaled,
diss = NULL,
distance = "euclidean",
min.nc = 2,
max.nc = 10,
method = "kmeans")
# try different clusters and compare the results
km2 <- kmeans(Auto_scale,2)
# try different clusters and compare the results
km2 <- kmeans(Auto_scaled,2)
remove(Auto_scale)
# try different clusters and compare the results
km2 <- kmeans(Auto_scaled,2)
km2 <- kmeans(Auto_scaled,2)
km3 <- kmeans(Auto_scaled,3)
km4 <- kmeans(Auto_scaled,4)
km5 <- kmeans(Auto_scaled,5)
km6 <- kmeans(Auto_scaled,6)
km7 <- kmeans(Auto_scaled,7)
# visualize the results
library(cowplot)
p1 <- fviz_cluster(km2,data = Auto_scaled) + theme_minimal() +
ggtitle("k=2")
p1
p1 <- fviz_cluster(km2,data = Auto_scaled) + theme_minimal() +
ggtitle("k=2")
p2 <- fviz_cluster(km3,data = Auto_scaled) + theme_minimal() +
ggtitle("k=3")
p3 <- fviz_cluster(km4,data = Auto_scaled) + theme_minimal() +
ggtitle("k=4")
p4 <- fviz_cluster(km5,data = Auto_scaled) + theme_minimal() +
ggtitle("k=5")
p5 <- fviz_cluster(km6,data = Auto_scaled) + theme_minimal() +
ggtitle("k=6")
p6 <- fviz_cluster(km7,data = Auto_scaled) + theme_minimal() +
ggtitle("k=7")
plot_grid(p1,p2,p3,p4,p5,p6,
labels = C("k2","k3","k4","k5","k6","k7"))
library(cowplot)
plot_grid(p1,p2,p3,p4,p5,p6)
# pick the best number of clusters
###nstart uses many iterations through kmeans to remove randomization of initial assignments
finalkm <- kmeans(Auto_scaled,3,nstart = 30)
fviz_cluster(finalkm,data = Auto_scaled)
fviz_cluster(finalkm)
fviz_cluster(finalkm,data = Auto_scaled)
# store the cluster information in the data frame
Auto$cluster <- as.factor(finalkm$cluster)
# use ggpairs to see if the clusters are meaningful
library(GGally)
ggpairs(Auto,1:5,mapping = aes(color = cluster,alpha = .5))
#------------------------------------------------
### PCA
#------------------------------------------------
# use the auto data set
data(Auto)
# run pca model
auto_pca <- prcomp(Auto)
# run pca model
auto_pca <- prcomp(Auto[,1:7])
# summary
summary(auto_pca)
# run pca model
auto_pca <- prcomp(Auto[,1:7],
center = TRUE,
scale = TRUE)
# summary
summary(auto_pca)
# how much variation each dimension captures
fviz_screeplot(auto_pca)
# Extract the results for variables
var <- get_pca_var(auto_pca)
# Contributions of variables to PC1
fviz_contrib(auto_pca,choice = "var",axes = 1,top = 10)
install.packages("devtools")
install.packages("devtools")
install_github("vqv/ggbiplot")
library(devtools)
install_github("vqv/ggbiplot")
#install.packages("devtools")
#library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
# ggbiplot to visualize the results
ggbiplot(auto_pca,ellipse = TRUE)
setwd("C:\\Users\\noahe\\Desktop\\MGSC310")
steam <- read.csv("steam.csv")
#####exploratory data analysis
###there are no rows with missing values
nrow(steam[!complete.cases(steam),])
###variable engineering/cleaning
#steam$pos_rating_ratio <- steam$positive_ratings/steam$negative_ratings
##turns the unites into dollars
steam$price <- steam$price *1.28
####got rid of cariables that wont really help us
steam<- subset(steam, select = -c(appid,english,steamspy_tags,
name,release_date,
platforms,publisher,developer))
####I got rid of all the ';' delimited variables and instead assigned them a single value for that columne
steam$genres <- do.call('rbind',strsplit(as.character(steam$genres), ';', fixed=TRUE))[,1]
steam$categories <- do.call('rbind',strsplit(as.character(steam$categories), ';', fixed=TRUE))[,1]
###turn them all into factors
steam$categories <- as.factor(steam$categories)
steam$genres <- as.factor(steam$genres)
###removing outliers
steam <- steam[steam$average_playtime < 100000,]
steam <- steam[steam$price <50,]
steam <- steam[steam$average_playtime < 40000,]
steam <- steam[steam$negative_ratings < 2e+05,]
steam <- steam[steam$positive_ratings < 1e+06,]
# Creating Groups of factored variables
library(forcats)
library(tidyverse)
###one way to do it
steam$simple_categories <- fct_lump(steam$categories,n = 4)
unique(steam$simple_categories)
steam$categories <- ifelse(steam$categories == "Single-player","SinglePlayer",
ifelse(steam$categories == "Multi-player","Multi-Player",
ifelse(steam$categories == "Online Multi-Player","Multi-Player",
ifelse(steam$categories == "Local Multi-Player","Multi-Player",
ifelse(steam$categories == "MMO","MMO",
ifelse(steam$categories == "Co-op","Co-op",
ifelse(steam$categories == "Shared/Split Screen","Co-op",
ifelse(steam$categories == "Local Co-op","Co-op",
ifelse(steam$categories == "Online Co-op","Co-op",
ifelse(steam$categories == "Steam Cloud","Steam",
ifelse(steam$categories == "Steam Trading Cards","Steam",
ifelse(steam$categories == "Steam Leaderboards","Steam",
ifelse(steam$categories == "Steam Achievements","Steam","Other")))))))))))))
steam$categories <- as.factor(steam$categories)
unique(steam$categories)
unique(steam$categories)
###reducing the number of genres
steam$genres <- fct_lump(steam$genres,n = 5)
unique(steam$genres)
###creates variable successful game to help predict by (I was thinking we could
#use this as our predicted variable)(1 is a successful game - over 1 million sold,0 is anything less)
steam$successfulGame <- ifelse(steam$owners == "10000000-20000000",1,
ifelse(steam$owners == "20000000-50000000",1,
ifelse(steam$owners == "50000000-100000000",1,
ifelse(steam$owners == "100000000-200000000",1,
ifelse(steam$owners == "5000000-10000000",1,
ifelse(steam$owners == "2000000-5000000",1,
ifelse(steam$owners == "1000000-2000000",1,0)))))))
steam$successfulGame <- as.factor(steam$successfulGame)
steam <- subset(steam, select = -c(owners,categories,positive_ratings,negative_ratings,
median_playtime))
steam$required_age <- as.factor(steam$required_age)
library(summarytools)
descr(steam)
set.seed(2019)
train_index <- sample(1:nrow(steam),.75*nrow(steam),replace = FALSE)
steam_train <- steam[train_index,]
steam_test <- steam[-train_index,]
dim(steam_train)
dim(steam_test)
####this was found to have non converging values
steam_logit <- glm(successfulGame~.,
data = steam_train,
family = "binomial")
summary(steam_logit)
steam_logit <- glm(successfulGame~price+factor(genres) + factor(required_age)+
average_playtime,
data = steam_train,
family = "binomial")
summary(steam_logit)
steam_train$logit_preds <- predict(steam_logit,type = "response")
steam_test$logit_preds <- predict(steam_logit,newdata = steam_test,
type = "response")
library(plotROC)
library(ggplot2)
####not working
ggplot(steam_train,aes(m = logit_preds,
d = successfulGame)) +
geom_roc(labelsize = 3.5,
cutoffs.at = c(.99,.9,.7,.6,.5,.4,.1,.01)) +
labs(title = "ROC curve for Train Steam Data",x = "False Positive Fraction",
y = "True Positive Fraction")
steam_train$logit_preds <- predict(steam_logit,type = "class")
steam_train$logit_preds <- predict(steam_logit,type = "response")
steam_train$logit_preds <- predict(steam_logit,type = "terms")
steam_train$logit_preds
steam_train$logit_preds <- predict(steam_logit,type = "response")
steam_test$logit_preds <- predict(steam_logit,newdata = steam_test,
type = "response")
range(steam_train$logit_preds)
