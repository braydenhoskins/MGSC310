84)
median(x)
mean(x)
max(x)
library(ggplot2)
library(factoextra)
#using iris data: flowers species
data("iris")
# hierarchical clustering
dissimilarity <- dist(irist[,3:4])
# hierarchical clustering
dissimilarity <- dist(irist[,c(3,4)])
# hierarchical clustering
dissimilarity <- dist(iris[,c(3,4)])
# hierarchical clustering
dissimilarity <- dist(iris[,c(3:4)])
clusters <- hclust(dissimilarity)
clusters
dissimilarity
# plotting the dendogram
plot(clusters)
rect.hclust(clusters,k=3)
# cutting the tree to find clusters
plot(clusters)
rect.hclust(clusters,k=3)
rect.hclust(clusters,k=3,border = 1:3)
rect.hclust(clusters,k=3,border = 1:3)
###cutting the dendrogram
cluster_cut <- cutree(clusters,k=3)
plot(ccluster_cut)
###cutting the dendrogram with specified number of clusters
cluster_cut <- cutree(clusters,k=3)
plot(ccluster_cut)
plot(cluster_cut)
table(ccluster_cut)
table(cluster_cut)
table(cluster_cut,iris$Species)
iris_clusters2 <- hculst(dist(iris[,3:4]),method = 'average')
iris_clusters2 <- hclust(dist(iris[,3:4]),method = 'average')
plot(iris_clusters2)
rect.hclust(iris_clusters2,3)
cluster_cut2 <- cutree(iris_clusters2,3)
table(cluster_cut2,iris$Species)
table(cluster_cut,iris$Species)
table(cluster_cut2,iris$Species)
ggplot(iris,aes(x= Petal.length,y = Petal.width,color = Species)) +
geom_point(alpha = .4,size = 3.5) +
geom_point(col = cluster_cut2)
ggplot(iris,aes(x= Peta.length,y = Petal.width,color = Species)) +
geom_point(alpha = .4,size = 3.5) +
geom_point(col = cluster_cut2)
colnames(iris)
ggplot(iris,aes(x= Petal.Length,y = Petal.width,color = Species)) +
geom_point(alpha = .4,size = 3.5) +
geom_point(col = cluster_cut2)
ggplot(iris,aes(x= Petal.Length,y = Petal.Width,color = Species)) +
geom_point(alpha = .4,size = 3.5) +
geom_point(col = cluster_cut2)
#------------------------------------------------
### Auto data
#------------------------------------------------
# using Auto data
library(ISLR)
data("Auto")
auto
# scaling
####make variables similar scales
str(Auto)
Auto_scale <- scale(Auto[,1:7])
head(Auto)
head(Auto_scale)
fviz_nbclust(Auto_scaled,
kmeans,
method="wss") +
geom_vline(xintercept=3, linetype =2) +
labs(subtitle ="Elbow Method")
Auto_scaled <- scale(Auto[,1:7]) ###gets rid of origin and name and scales the values
#------------------------------------------------
### number of clusters?
#------------------------------------------------
# elbow method
fviz_nbclust(Auto_scaled,
kmeans,
method="wss") +
geom_vline(xintercept=3, linetype =2) +
labs(subtitle ="Elbow Method")
# Silhouette method
fviz_nbclust(Auto_scaled,
kmeans,
method="silhouette")
# Gap Statistic method
fviz_nbclust(Auto_scaled,
kmeans,
method="gap_stat",
nboot=100)
library("NbClust")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "eucladian")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "euclidean")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "eucledean")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "eucladean")
Nb_cl <- NbClust(Auto_Scaled,
diss = NULL,
distance = "euclidean",
min.nc = 2,
max.nc = 10,
method = "kmeans")
Auto_scaled <- scale(Auto[,1:7]) ###gets rid of origin and name and scales the values
Nb_cl <- NbClust(Auto_scaled,
diss = NULL,
distance = "euclidean",
min.nc = 2,
max.nc = 10,
method = "kmeans")
# try different clusters and compare the results
km2 <- kmeans(Auto_scale,2)
# try different clusters and compare the results
km2 <- kmeans(Auto_scaled,2)
remove(Auto_scale)
# try different clusters and compare the results
km2 <- kmeans(Auto_scaled,2)
km2 <- kmeans(Auto_scaled,2)
km3 <- kmeans(Auto_scaled,3)
km4 <- kmeans(Auto_scaled,4)
km5 <- kmeans(Auto_scaled,5)
km6 <- kmeans(Auto_scaled,6)
km7 <- kmeans(Auto_scaled,7)
# visualize the results
library(cowplot)
p1 <- fviz_cluster(km2,data = Auto_scaled) + theme_minimal() +
ggtitle("k=2")
p1
p1 <- fviz_cluster(km2,data = Auto_scaled) + theme_minimal() +
ggtitle("k=2")
p2 <- fviz_cluster(km3,data = Auto_scaled) + theme_minimal() +
ggtitle("k=3")
p3 <- fviz_cluster(km4,data = Auto_scaled) + theme_minimal() +
ggtitle("k=4")
p4 <- fviz_cluster(km5,data = Auto_scaled) + theme_minimal() +
ggtitle("k=5")
p5 <- fviz_cluster(km6,data = Auto_scaled) + theme_minimal() +
ggtitle("k=6")
p6 <- fviz_cluster(km7,data = Auto_scaled) + theme_minimal() +
ggtitle("k=7")
plot_grid(p1,p2,p3,p4,p5,p6,
labels = C("k2","k3","k4","k5","k6","k7"))
library(cowplot)
plot_grid(p1,p2,p3,p4,p5,p6)
# pick the best number of clusters
###nstart uses many iterations through kmeans to remove randomization of initial assignments
finalkm <- kmeans(Auto_scaled,3,nstart = 30)
fviz_cluster(finalkm,data = Auto_scaled)
fviz_cluster(finalkm)
fviz_cluster(finalkm,data = Auto_scaled)
# store the cluster information in the data frame
Auto$cluster <- as.factor(finalkm$cluster)
# use ggpairs to see if the clusters are meaningful
library(GGally)
ggpairs(Auto,1:5,mapping = aes(color = cluster,alpha = .5))
#------------------------------------------------
### PCA
#------------------------------------------------
# use the auto data set
data(Auto)
# run pca model
auto_pca <- prcomp(Auto)
# run pca model
auto_pca <- prcomp(Auto[,1:7])
# summary
summary(auto_pca)
# run pca model
auto_pca <- prcomp(Auto[,1:7],
center = TRUE,
scale = TRUE)
# summary
summary(auto_pca)
# how much variation each dimension captures
fviz_screeplot(auto_pca)
# Extract the results for variables
var <- get_pca_var(auto_pca)
# Contributions of variables to PC1
fviz_contrib(auto_pca,choice = "var",axes = 1,top = 10)
install.packages("devtools")
install.packages("devtools")
install_github("vqv/ggbiplot")
library(devtools)
install_github("vqv/ggbiplot")
#install.packages("devtools")
#library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
# ggbiplot to visualize the results
ggbiplot(auto_pca,ellipse = TRUE)
setwd("C:\\Users\\noahe\\Desktop\\MGSC310")
steam <- read.csv("steam.csv")
###there are no rows with missing values
nrow(steam[!complete.cases(steam),])
###variable engineering/cleaning
#steam$pos_rating_ratio <- steam$positive_ratings/steam$negative_ratings
##turns the unites into dollars
steam$price <- steam$price *1.28
####got rid of cariables that wont really help us
steam<- subset(steam, select = -c(appid,english,steamspy_tags,
name,release_date,
platforms,publisher,developer))
steam$genres <- do.call('rbind',strsplit(as.character(steam$genres), ';', fixed=TRUE))[,1]
steam$categories <- do.call('rbind',strsplit(as.character(steam$categories), ';', fixed=TRUE))[,1]
###turn them all into factors
steam$categories <- as.factor(steam$categories)
steam$genres <- as.factor(steam$genres)
###removing outliers
steam <- steam[steam$average_playtime < 100000,]
steam <- steam[steam$price <50,]
steam <- steam[steam$average_playtime < 40000,]
steam <- steam[steam$negative_ratings < 2e+05,]
steam <- steam[steam$positive_ratings < 1e+06,]
steam <- read.csv("steam.csv")
unique(steam$publisher)
nrow(steam[!complete.cases(steam),])
###variable engineering/cleaning
#steam$pos_rating_ratio <- steam$positive_ratings/steam$negative_ratings
##turns the unites into dollars
steam$price <- steam$price *1.28
####got rid of cariables that wont really help us
steam<- subset(steam, select = -c(appid,english,steamspy_tags,
name,release_date,
platforms,publisher,developer))
####I got rid of all the ';' delimited variables and instead assigned them a single value for that columne
steam$genres <- do.call('rbind',strsplit(as.character(steam$genres), ';', fixed=TRUE))[,1]
steam$categories <- do.call('rbind',strsplit(as.character(steam$categories), ';', fixed=TRUE))[,1]
###turn them all into factors
steam$categories <- as.factor(steam$categories)
steam$genres <- as.factor(steam$genres)
###removing outliers
steam <- steam[steam$average_playtime < 100000,]
steam <- steam[steam$price <50,]
steam <- steam[steam$average_playtime < 40000,]
steam <- steam[steam$negative_ratings < 2e+05,]
steam <- steam[steam$positive_ratings < 1e+06,]
###one way to do it
steam$simple_categories <- fct_lump(steam$categories,n = 4)
# Creating Groups of factored variables
library(forcats)
library(tidyverse)
###one way to do it
steam$simple_categories <- fct_lump(steam$categories,n = 4)
###reducing the number of genres
steam$genres <- fct_lump(steam$genres,n = 5)
unique(steam$genres)
unique(simple_categories)
unique(steam$simple_categories)
steam$categories <- ifelse(steam$categories == "Single-player","SinglePlayer",
ifelse(steam$categories == "Multi-player","Multi-Player",
ifelse(steam$categories == "Online Multi-Player","Multi-Player",
ifelse(steam$categories == "Local Multi-Player","Multi-Player",
ifelse(steam$categories == "MMO","MMO",
ifelse(steam$categories == "Co-op","Co-op",
ifelse(steam$categories == "Shared/Split Screen","Co-op",
ifelse(steam$categories == "Local Co-op","Co-op",
ifelse(steam$categories == "Online Co-op","Co-op",
ifelse(steam$categories == "Steam Cloud","Steam",
ifelse(steam$categories == "Steam Trading Cards","Steam",
ifelse(steam$categories == "Steam Leaderboards","Steam",
ifelse(steam$categories == "Steam Achievements","Steam","Other")))))))))))))
steam$categories <- as.factor(steam$categories)
unique(steam$categories)
###reducing the number of genres
steam$genres <- fct_lump(steam$genres,n = 5)
unique(steam$genres)
###creates variable successful game to help predict by (I was thinking we could
#use this as our predicted variable)(1 is a successful game - over 1 million sold,0 is anything less)
steam$successfulGame <- ifelse(steam$owners == "10000000-20000000",1,
ifelse(steam$owners == "20000000-50000000",1,
ifelse(steam$owners == "50000000-100000000",1,
ifelse(steam$owners == "100000000-200000000",1,
ifelse(steam$owners == "5000000-10000000",1,
ifelse(steam$owners == "2000000-5000000",1,
ifelse(steam$owners == "1000000-2000000",1,0)))))))
table(steam$categories)
table(steam$categories,steam$successfulGame)
steam$successfulGame <- as.factor(steam$successfulGame)
steam <- subset(steam, select = -c(owners,categories))
steam$required_age <- as.factor(steam$required_age)
library(summarytools)
descr(steam)
colnames(steam)
steam <- subset(steam, select = -c(owners,categories,positive_ratings,negative_ratings,
median_playtime))
<<<<<<< HEAD
###Summary Statistics R code by Noah Estrada-Rand, Brady Hoskins, Charles Filce
setwd("C:\\Users\\noahe\\Desktop\\MGSC310")
steam <- read.csv("steam.csv")
#####exploratory data analysis
###there are no rows with missing values
nrow(steam[!complete.cases(steam),])
###variable engineering/cleaning
#steam$pos_rating_ratio <- steam$positive_ratings/steam$negative_ratings
##turns the unites into dollars
steam$price <- steam$price *1.28
####got rid of cariables that wont really help us
steam<- subset(steam, select = -c(appid,english,steamspy_tags,
name,release_date,
platforms,publisher,developer))
####I got rid of all the ';' delimited variables and instead assigned them a single value for that columne
steam$genres <- do.call('rbind',strsplit(as.character(steam$genres), ';', fixed=TRUE))[,1]
steam$categories <- do.call('rbind',strsplit(as.character(steam$categories), ';', fixed=TRUE))[,1]
###turn them all into factors
steam$categories <- as.factor(steam$categories)
steam$genres <- as.factor(steam$genres)
###removing outliers
steam <- steam[steam$average_playtime < 100000,]
steam <- steam[steam$price <50,]
steam <- steam[steam$average_playtime < 40000,]
steam <- steam[steam$negative_ratings < 2e+05,]
steam <- steam[steam$positive_ratings < 1e+06,]
# Creating Groups of factored variables
library(forcats)
library(tidyverse)
###one way to do it
steam$simple_categories <- fct_lump(steam$categories,n = 4)
unique(steam$simple_categories)
steam$categories <- ifelse(steam$categories == "Single-player","SinglePlayer",
ifelse(steam$categories == "Multi-player","Multi-Player",
ifelse(steam$categories == "Online Multi-Player","Multi-Player",
ifelse(steam$categories == "Local Multi-Player","Multi-Player",
ifelse(steam$categories == "MMO","MMO",
ifelse(steam$categories == "Co-op","Co-op",
ifelse(steam$categories == "Shared/Split Screen","Co-op",
ifelse(steam$categories == "Local Co-op","Co-op",
ifelse(steam$categories == "Online Co-op","Co-op",
ifelse(steam$categories == "Steam Cloud","Steam",
ifelse(steam$categories == "Steam Trading Cards","Steam",
ifelse(steam$categories == "Steam Leaderboards","Steam",
ifelse(steam$categories == "Steam Achievements","Steam","Other")))))))))))))
steam$categories <- as.factor(steam$categories)
unique(steam$categories)
unique(steam$categories)
###reducing the number of genres
steam$genres <- fct_lump(steam$genres,n = 5)
unique(steam$genres)
###creates variable successful game to help predict by (I was thinking we could
#use this as our predicted variable)(1 is a successful game - over 1 million sold,0 is anything less)
steam$successfulGame <- ifelse(steam$owners == "10000000-20000000",1,
ifelse(steam$owners == "20000000-50000000",1,
ifelse(steam$owners == "50000000-100000000",1,
ifelse(steam$owners == "100000000-200000000",1,
ifelse(steam$owners == "5000000-10000000",1,
ifelse(steam$owners == "2000000-5000000",1,
ifelse(steam$owners == "1000000-2000000",1,0)))))))
steam$successfulGame <- as.factor(steam$successfulGame)
steam <- subset(steam, select = -c(owners,categories,positive_ratings,negative_ratings,
median_playtime))
steam$required_age <- as.factor(steam$required_age)
library(summarytools)
descr(steam)
colnames(steam)
#####
set.seed(2019)
train_index <- sample(1:nrow(steam),.75*nrow(steam),replace = FALSE)
steam_train <- steam[train_index,]
steam_test <- steam[-train_index,]
dim(steam_train)
dim(steam_test)
####this was found to have non converging values
steam_logit <- glm(successfulGame~.,
data = steam_train,
family = "binomial")
summary(steam_logit)
####the issue was the positive and negative ratings
steam_logit <- glm(successfulGame~price+factor(genres) + factor(required_age)+
average_playtime,
data = steam_train,
family = "binomial")
summary(steam_logit)
steam_train$logit_preds <- predict(steam_logit,type = "response")
steam_test$logit_preds <- predict(steam_logit,newdata = steam_test,
type = "response")
####not working
train_ROC <- ggplot(steam_train,aes(m = logit_preds,
d = successfulGame)) +
geom_roc(labelsize = 3.5,
cutoffs.at = c(.99,.9,.7,.6,.5,.4,.1,.01)) +
labs(title = "ROC Curve for Train Data",x = "False Positive Fraction",
y= "True Positive Fraction")
library(plotROC)
library(ggplot2)
####not working
train_ROC <- ggplot(steam_train,aes(m = logit_preds,
d = successfulGame)) +
geom_roc(labelsize = 3.5,
cutoffs.at = c(.99,.9,.7,.6,.5,.4,.1,.01)) +
labs(title = "ROC Curve for Train Data",x = "False Positive Fraction",
y= "True Positive Fraction")
train_ROC
steam_train$logit_preds == Inf
sum(steam_train$logit_preds == Inf)
preds_train <- data.frame(steam_train,predictions = predict(steam_logit,type = "response"))
preds_test <- data.frame(steam_test,predictions = predict(steam_logit,newdata = steam_test,
type = "response"))
inSampleROC <- ggplot(preds_train,aes(m = predictions,
d = successfulGame)) +
geom_roc(labelsize = 3.5,
cutoffs.at = c(.99,.9,.7,.6,.5,.4,.1,.01)) +
labs(title = "ROC Curve for In-Sample Predictions",x = "False Positive Fraction",
y= "True Positive Fraction")
inSampleROC
####dimensionality reduction
####Dont run this, it takes too long, we need to use other dimensionality reduction techniques
library(glmnet)
library(glmnetUtils)
####### finding the best mtry to use
set.seed(2019)
train_idx <- sample(1:nrow(steam), size = floor(.75 * nrow(steam)))
steam_train <- steam[train_idx,]
steam_test <- steam[-train_idx,]
library(randomForest)
rf_mods <- list()
oob_err <- NULL##to store out of bag error
test_err <- NULL
for(mtry in 1:9){
rf_fit <- randomForest(successfulGame~.,
data = steam_train,
mtry = mtry,
ntrees = 500,
type = classification)
oob_err[mtry] <- rf_fit$err.rate[500]
}
results_df <- data.frame(mtry = 1:9,
oob_err)
ggplot(results_df,aes(x = mtry,y = oob_err)) + geom_point() +geom_line()
####bagging for randomforest
steam_bagged <- randomForest(successfulGame~.,
data = steam_train,
mtry = 9,
ntrees = 500,
type = classification,
importance = TRUE)
bagged_preds <- predict(steam_bagged,type = "response")
table(bagged_preds)
preds_train_bagged <- data.frame(steam_train,bag_preds = bagged_preds)
preds_test_bagged <-data.frame(steam_test,bag_preds = predict(steam_bagged,
newdata = steam_test,
type = "response"))
library(gmodels)
###training confusion matrix
CrossTable(preds_train_bagged$successfulGame,preds_train_bagged$bag_preds,
prop.r = FALSE,
prop.c = FALSE,
prop.t = FALSE,
prop.chisq = FALSE)
(104+19635)/20157
###test confusion matrix
CrossTable(preds_test_bagged$successfulGame,preds_test_bagged$bag_preds,
prop.r = FALSE,
prop.c = FALSE,
prop.t = FALSE,
prop.chisq = FALSE)
random_forest_steam <- randomForest(successfulGame~.,
data = steam_train,
mtry = 4,
ntrees = 500,
type = classification,
importance = TRUE)
random_forest_preds <- predict(random_forest_steam,type = "response")
preds_2 <- data.frame(steam_train,preds= random_forest_preds)
preds_test_2 <-data.frame(steam_test,preds = predict(random_forest_steam,
newdata = steam_test,
type = "response"))
#importance check
importance(random_forest_steam)
plot(random_forest_steam)
varImpPlot(random_forest_steam)
varImpPlot(steam_bagged)
###tree based on the top 4 most important variables
library(tree)
steam_tree <- tree(successfulGame~.,
data = steam_train)
plot(steam_tree)
text(steam_tree,pretty = 0)
tree_cv <- cv.tree(steam_tree)
best_tree_index <- which.min(tree_cv$dev)
tree_cv$size[best_tree_index]
####same as above, thus no pruning needed
pruned_tree <- prune.tree(steam_tree,best = 6)
plot(pruned_tree)
text(pruned_tree,pretty=0)
###performance
basic_preds_train <- data.frame(steam_train,preds = predict(steam_tree,
type = "class"))
basic_preds_test <- data.frame(steam_test,preds = predict(steam_tree,
newdata = steam_test,
type = "class"))
###for train
library(gmodels)
CrossTable(basic_preds_train$successfulGame,basic_preds_train$preds,
prop.r = FALSE,
prop.c = FALSE,
prop.t = FALSE,
prop.chisq = FALSE)
###for test
CrossTable(basic_preds_test$successfulGame, basic_preds_test$preds,
prop.r = FALSE,
prop.c = FALSE,
prop.t = FALSE,
prop.chisq = FALSE)
